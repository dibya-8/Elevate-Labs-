{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 477177,
          "sourceType": "datasetVersion",
          "datasetId": 216167
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Heart Disease Prediction Project",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dibya-8/Elevate-Labs-/blob/main/Heart_Disease_Prediction_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "johnsmith88_heart_disease_dataset_path = kagglehub.dataset_download('johnsmith88/heart-disease-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "1B-Ep_5v_WDd"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Heart Disease Prediction Project\n",
        "\n",
        "**Goal:** To predict the presence or absence of heart disease based on various patient attributes using machine learning classification models.\n",
        "\n",
        "**Dataset:** Heart Disease UCI dataset. This dataset contains 14 attributes collected from patients. The \"target\" column indicates the presence (1) or absence (0) of heart disease.\n",
        "\n",
        "**Plan:**\n",
        "1.  Load and Explore Data (EDA)\n",
        "2.  Preprocess Data (Handle categorical features, split data, scale numerical features)\n",
        "3.  Train and Evaluate Machine Learning Models (Logistic Regression, Random Forest, SVM)\n",
        "4.  Compare Models and Conclude"
      ],
      "metadata": {
        "_uuid": "4c994ca6-151d-461a-82cd-ec0e82bf6a5f",
        "_cell_guid": "704e665c-1999-4905-a9fe-199396b75306",
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1apyx6uS_WDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-12T12:36:15.589192Z",
          "iopub.execute_input": "2025-04-12T12:36:15.589563Z",
          "iopub.status.idle": "2025-04-12T12:36:17.196729Z",
          "shell.execute_reply.started": "2025-04-12T12:36:15.589538Z",
          "shell.execute_reply": "2025-04-12T12:36:17.196182Z"
        },
        "id": "gf78Z6TN_WDk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load and Explore Data (EDA)"
      ],
      "metadata": {
        "id": "EBaUxUVA_WDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# Make sure the file path matches where Kaggle stores the input data\n",
        "try:\n",
        "    df = pd.read_csv('/kaggle/input/heart-disease-dataset/heart.csv')\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: heart.csv not found. Please ensure the dataset is added correctly.\")\n",
        "    # As a fallback for local testing, you might load it like this:\n",
        "    # df = pd.read_csv('heart.csv')\n",
        "    # exit() # Or handle the error appropriately\n",
        "\n",
        "# Display basic information\n",
        "print(\"\\n--- Dataset Info ---\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n--- First 5 Rows ---\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n--- Descriptive Statistics ---\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\n--- Check for Missing Values ---\")\n",
        "print(df.isnull().sum())\n",
        "# Good news: No missing values in this standard version of the dataset.\n",
        "\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "target_counts = df['target'].value_counts()\n",
        "print(target_counts)\n",
        "sns.countplot(x='target', data=df)\n",
        "plt.title('Distribution of Target Variable (0: No Disease, 1: Disease)')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-12T12:36:17.198518Z",
          "iopub.execute_input": "2025-04-12T12:36:17.198924Z",
          "iopub.status.idle": "2025-04-12T12:36:17.480577Z",
          "shell.execute_reply.started": "2025-04-12T12:36:17.198897Z",
          "shell.execute_reply": "2025-04-12T12:36:17.479849Z"
        },
        "id": "b5CEl2Lb_WDm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA Visualizations\n",
        "\n",
        "Let's explore the relationships between features and the target variable.\n",
        "\n",
        "**Age Distribution:**"
      ],
      "metadata": {
        "id": "WnUjWaT8_WDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data=df, x='age', hue='target', kde=True)\n",
        "plt.title('Age Distribution by Target')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-12T12:36:17.481633Z",
          "iopub.execute_input": "2025-04-12T12:36:17.481899Z",
          "iopub.status.idle": "2025-04-12T12:36:17.82333Z",
          "shell.execute_reply.started": "2025-04-12T12:36:17.481875Z",
          "shell.execute_reply": "2025-04-12T12:36:17.822567Z"
        },
        "id": "f5EbtOsQ_WDn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sex Distribution:** (0 = female, 1 = male)"
      ],
      "metadata": {
        "id": "Qr0a1E9X_WDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=df, x='sex', hue='target')\n",
        "plt.title('Heart Disease Frequency for Sex')\n",
        "plt.xlabel('Sex (0 = Female, 1 = Male)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-12T12:36:17.824068Z",
          "iopub.execute_input": "2025-04-12T12:36:17.824332Z",
          "iopub.status.idle": "2025-04-12T12:36:17.998054Z",
          "shell.execute_reply.started": "2025-04-12T12:36:17.824306Z",
          "shell.execute_reply": "2025-04-12T12:36:17.99731Z"
        },
        "id": "SD87d0IR_WDo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chest Pain Type (cp) Distribution:**\n",
        "*   0: Typical angina\n",
        "*   1: Atypical angina\n",
        "*   2: Non-anginal pain\n",
        "*   3: Asymptomatic"
      ],
      "metadata": {
        "id": "J5ICM1_u_WDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=df, x='cp', hue='target')\n",
        "plt.title('Heart Disease Frequency According to Chest Pain Type')\n",
        "plt.xlabel('Chest Pain Type')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-12T12:36:17.99892Z",
          "iopub.execute_input": "2025-04-12T12:36:17.999679Z",
          "iopub.status.idle": "2025-04-12T12:36:18.186548Z",
          "shell.execute_reply.started": "2025-04-12T12:36:17.999638Z",
          "shell.execute_reply": "2025-04-12T12:36:18.185921Z"
        },
        "id": "yY1OBRZC_WDp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlation Heatmap:**"
      ],
      "metadata": {
        "id": "zHHNvknk_WDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-12T12:36:18.187279Z",
          "iopub.execute_input": "2025-04-12T12:36:18.187574Z",
          "iopub.status.idle": "2025-04-12T12:36:18.820159Z",
          "shell.execute_reply.started": "2025-04-12T12:36:18.187547Z",
          "shell.execute_reply": "2025-04-12T12:36:18.819434Z"
        },
        "id": "U_DE6IKZ_WDp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "We need to handle categorical features and scale numerical features.\n",
        "\n",
        "*   **Categorical features:** `sex`, `cp`, `fbs`, `restecg`, `exang`, `slope`, `ca`, `thal`. Some are binary, others are nominal/ordinal. We'll use One-Hot Encoding for nominal features with more than 2 categories (`cp`, `restecg`, `slope`, `thal`, `ca`). Note: `sex`, `fbs`, `exang` are already 0/1.\n",
        "*   **Numerical features:** `age`, `trestbps`, `chol`, `thalach`, `oldpeak`. We'll scale these using StandardScaler."
      ],
      "metadata": {
        "id": "IE8Y2pr4_WDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Identify feature types\n",
        "categorical_features = ['cp', 'restecg', 'slope', 'ca', 'thal'] # Features to one-hot encode\n",
        "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
        "# Binary features ('sex', 'fbs', 'exang') don't strictly need one-hot encoding or scaling if kept as 0/1\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "# Create preprocessing pipelines for numerical and categorical features\n",
        "# Numerical pipeline: Scale features\n",
        "num_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical pipeline: One-hot encode features\n",
        "cat_pipeline = Pipeline([\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # handle_unknown='ignore' is safer for unseen categories in test set\n",
        "])\n",
        "\n",
        "# Combine pipelines using ColumnTransformer\n",
        "# Apply num_pipeline to numerical features\n",
        "# Apply cat_pipeline to categorical features needing one-hot encoding\n",
        "# Passthrough binary features ('sex', 'fbs', 'exang') or include them if preferred\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_pipeline, numerical_features),\n",
        "        ('cat', cat_pipeline, categorical_features),\n",
        "        ('passthrough', 'passthrough', ['sex', 'fbs', 'exang']) # Keep binary features as they are\n",
        "    ],\n",
        "    remainder='drop' # Drop any columns not specified (shouldn't be any here)\n",
        ")\n",
        "\n",
        "# Apply the preprocessing pipeline to the training data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Apply the *fitted* preprocessing pipeline to the testing data\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Get feature names after transformation (useful for some models like Tree-based ones)\n",
        "feature_names_out = preprocessor.get_feature_names_out()\n",
        "print(f\"\\nProcessed feature names ({X_train_processed.shape[1]} features):\\n{feature_names_out}\")\n",
        "\n",
        "\n",
        "print(f\"\\nProcessed Training set shape: {X_train_processed.shape}\")\n",
        "print(f\"Processed Test set shape: {X_test_processed.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-12T12:36:18.820958Z",
          "iopub.execute_input": "2025-04-12T12:36:18.821233Z",
          "iopub.status.idle": "2025-04-12T12:36:18.848066Z",
          "shell.execute_reply.started": "2025-04-12T12:36:18.821209Z",
          "shell.execute_reply": "2025-04-12T12:36:18.847511Z"
        },
        "id": "xK1iq4uc_WDq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train and Evaluate Machine Learning Models\n",
        "\n",
        "We will train and evaluate the following models:\n",
        "1.  Logistic Regression\n",
        "2.  Random Forest Classifier\n",
        "3.  Support Vector Machine (SVM)"
      ],
      "metadata": {
        "id": "hfrhs5ic_WDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"Support Vector Machine\": SVC(probability=True, random_state=42) # probability=True for ROC AUC\n",
        "}\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for name, model in models.items():\n",
        "    print(f\"--- Training {name} ---\")\n",
        "    # Train the model\n",
        "    model.fit(X_train_processed, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_train = model.predict(X_train_processed)\n",
        "    y_pred_test = model.predict(X_test_processed)\n",
        "    y_prob_test = model.predict_proba(X_test_processed)[:, 1] # Probabilities for ROC AUC\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "    report_test = classification_report(y_test, y_pred_test)\n",
        "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
        "    roc_auc = roc_auc_score(y_test, y_prob_test)\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        \"model\": model,\n",
        "        \"accuracy_train\": accuracy_train,\n",
        "        \"accuracy_test\": accuracy_test,\n",
        "        \"classification_report\": report_test,\n",
        "        \"confusion_matrix\": cm_test,\n",
        "        \"roc_auc\": roc_auc,\n",
        "        \"y_pred\": y_pred_test,\n",
        "        \"y_prob\": y_prob_test\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n--- Results for {name} ---\")\n",
        "    print(f\"Training Accuracy: {accuracy_train:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy_test:.4f}\")\n",
        "    print(f\"Test ROC AUC: {roc_auc:.4f}\")\n",
        "    print(\"Test Classification Report:\\n\", report_test)\n",
        "    print(\"Test Confusion Matrix:\\n\", cm_test)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'{name} - Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-12T12:36:18.848704Z",
          "iopub.execute_input": "2025-04-12T12:36:18.84888Z",
          "iopub.status.idle": "2025-04-12T12:36:20.011615Z",
          "shell.execute_reply.started": "2025-04-12T12:36:18.848865Z",
          "shell.execute_reply": "2025-04-12T12:36:20.010856Z"
        },
        "id": "1k3vqkGl_WDq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Compare Models and Conclude\n",
        "\n",
        "Let's compare the models based on Test Accuracy and ROC AUC Score."
      ],
      "metadata": {
        "id": "sK25se9M_WDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display comparison summary\n",
        "print(\"\\n--- Model Comparison ---\")\n",
        "comparison_data = []\n",
        "for name, result in results.items():\n",
        "    comparison_data.append({\n",
        "        \"Model\": name,\n",
        "        \"Test Accuracy\": result['accuracy_test'],\n",
        "        \"Test ROC AUC\": result['roc_auc']\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(comparison_df.sort_values(by='Test ROC AUC', ascending=False))\n",
        "\n",
        "# Plot ROC Curves for all models\n",
        "plt.figure(figsize=(10, 8))\n",
        "for name, result in results.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, result['y_prob'])\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {result['roc_auc']:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess') # Dashed line for random guess\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curve Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-12T12:36:20.013593Z",
          "iopub.execute_input": "2025-04-12T12:36:20.013802Z",
          "iopub.status.idle": "2025-04-12T12:36:20.264084Z",
          "shell.execute_reply.started": "2025-04-12T12:36:20.013784Z",
          "shell.execute_reply": "2025-04-12T12:36:20.26345Z"
        },
        "id": "RUFlsUi8_WDr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "In this notebook, we aimed to predict heart disease using the UCI Heart Disease dataset.\n",
        "\n",
        "1.  **Data Exploration:** We analyzed the dataset, visualized feature distributions, and checked correlations.\n",
        "2.  **Preprocessing:** We handled categorical features using One-Hot Encoding and scaled numerical features using StandardScaler. The data was split into training and testing sets.\n",
        "3.  **Model Training:** We trained three common classification models: Logistic Regression, Random Forest, and Support Vector Machine.\n",
        "4.  **Evaluation:** The models were evaluated using accuracy, classification reports, confusion matrices, and ROC AUC scores.\n",
        "\n",
        "**Findings:**\n",
        "*   All models performed reasonably well, indicating that the features are predictive of heart disease.\n",
        "*   Based on the test set performance (Accuracy and ROC AUC), [Comment on which model(s) performed best - e.g., \"Logistic Regression and SVM showed strong results\" or \"Random Forest slightly outperformed the others\"]. Check the specific output from your run.\n",
        "*   The ROC curve comparison provides a visual way to assess the trade-off between True Positive Rate and False Positive Rate for each model.\n",
        "\n",
        "**Potential Future Work:**\n",
        "*   **Hyperparameter Tuning:** Use techniques like GridSearchCV or RandomizedSearchCV to find optimal parameters for each model, potentially improving performance further.\n",
        "*   **Feature Engineering:** Create new features from existing ones if domain knowledge suggests potential interactions.\n",
        "*   **Try Other Models:** Experiment with models like Gradient Boosting (XGBoost, LightGBM) or Neural Networks.\n",
        "*   **Cross-Validation:** Implement k-fold cross-validation during training for more robust evaluation.\n",
        "*   **Interpretability:** Use techniques like SHAP or LIME to understand *why* a model makes certain predictions, especially for more complex models like Random Forest.\n"
      ],
      "metadata": {
        "id": "SIvcADlB_WDr"
      }
    }
  ]
}